*********************************
* UNDERSTANDING TIME COMPLEXITY *
*********************************
Time complexity is the amount of time a certain piece of code is executed in a loop.
It is rate of growth of time with respect to the inputs taken during the program execution.

NOTE: 
1) Time Complexity of algorithm/code is not equal to the actual time required to execute a particular code but the number of times a statement executes.
2) Actual time requires to execute code is machine dependent and also it considers network load if your machine is in LAN/WAN.

Order of growth is how the time of execution(not time of execution) depends on the length of the input.

Time complexity is represented generally by O(big-oh)-notation (Though we have big omega and big theta notation).
While analysing an algorithm, we mostly consider O-notation because it will give us an upper limit of the execution time i.e. the execution time in the worst case. 

Eg 1:

#include <stdio.h>
void main()
{
    int i, n = 8;
    for (i = 1; i <= n; i++) {
        printf("Hello World !!!\n");
    }
}

prints Hello World!!! 10 times in a loop, that means for anything inside the loop running from i<=n, the time complexity is O(N).

Eg 2:

#include <stdio.h>
void main(){
    cout<<"Hello World !!!";
}

prints Hello World !!! only one time and it is not executed again and again and hence the time complexity is O(1).

Eg 3: 

#include <stdio.h>
void main(){
    int matrixA[10][10];
    int rows=10,cols=10;
    int sum=0
    for(int i=0;i<rows;++i){
        for(int j=0;j<cols;++j){
            sum+=matrixA[i][j];
        }
    }
}

In this example sum+=MatrixA[i][j] runs for cols*rows times, hence the time_complexity is O(rows*cols).
If it were a square matrix then time_complexity is O(n^2), where n is size of matrix.
Even searching for an element in nxn matrix is of order O(n^2).


***************************
* NEED OF TIME COMPLEXITY *
***************************
Using the above approach, if we wanted to find the sum of n x n x n.....(t times), then time complexity would be O(n^t) which could be a very very big number.
So we need a better algorithm instead of this traditionally algorithm.

So to analyse different algorithms independent of machine time, programming style, etc., we use time complexity.


************************************************************
* FIND TIME COMPLEXITY FOR EACH OF THE FOLLOWING QUESTIONS *
************************************************************

Question-1
----------

int a = 0;
for (i = 0; i < N; i++) {
    for (j = N; j > i; j--) {
        a = a + i + j;
    }
}

Ans.    a=a+i+j is getting executed N times when i=0, then N-1 times when i=1, ....
        Time Complexity=O(N+(N-1)+(N-2)+...+2+1)
        Time Complexity=O(N(N+1)/2)

Question-2
----------

int a = 0, b = 0;
for (i = 0; i < N; i++) {
    a = a + rand();
}
for (j = 0; j < M; j++) {
    b = b + rand();
}

Ans.    Time Complexity=O(N+M) (As a=a+randn() is getting executed N times and b=b+randn() is getting executed M times)

Question-3
----------

int i, j, k = 0;
for (i = n / 2; i <= n; i++) {
    for (j = 2; j <= n; j = j * 2) {
        k = k + n / 2;
    }
}

Ans.    k=k+n/2 is getting executed log(n)_base(2)[From j loop]*(n/2)[From i loop]
        So Time Complexity=O( (n*log(n)_base(2))/2 )

Question-4
----------

int a = 0, i = N;
while (i > 0) {
    a += i;
    i /= 2;
}

Ans.    a+=i is the statement which is executed again and again in the loop.
        So Time Complexity=O(log(n)_base(2))


**************
* O-notation *
**************
To denote asymptotic upper bound, we use O-notation.

For a given function g(n), we denote by  O(g(n))(pronounced “big-oh of g of n”) the set of functions:
O(g(n)) = { f(n) : there exist positive constants c and n0 such that 0<=f(n)<=c*g(n) for all n>=n0 }

**************
* Ω-notation *
**************
To denote asymptotic lower bound, we use Ω-notation.

For a given function g(n), we denote by  Ω(g(n))(pronounced “big-omega of g of n”) the set of functions:
Ω(g(n)) = { f(n) : there exist positive constants c and n0 such that 0<=c*g(n)<=f(n) for all n>=n0 }

**************
* θ-notation *
**************
To denote asymptotic tight bound, we use θ-notation.

For a given function g(n), we denote by  θ(g(n))(pronounced “big-theta of g of n”) the set of functions:
θ(g(n)) = { f(n) : there exist positive constants c1,c2 and n0 such that 0<=c1*g(n)<=f(n)<=c2*g(n) for all n>=n0 }



From Best to Worst case scenerio, the algorithm with increasing runnning time complexity is in the order:

O(log(n)_base(2)) < O(n) < O(n*log(n)_base(2)) < O(n^c) < O(c^n) < O(n!)
(here c is a constant and data input is expressed in form of n):